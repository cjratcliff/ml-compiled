""""""""""""""""""""""""""""""""""""""""
Explore-exploit dilemma
""""""""""""""""""""""""""""""""""""""""

Acquisition functions
-------------------------
An acquisition function defines how to balance exploration and exploitation when deciding the next point to sample while trying to maximize the cumulative reward.

https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf

Probability of improvement
'''''''''''''''''''''''''''
Pick the action which maximises the chance of getting to a state with a value greater than the current best state.

Expected improvement
''''''''''''''''''''''
Pick the action which maximises the expected improvement of that new state over the current best state.

Upper confidence bound
'''''''''''''''''''''''''''

Boltzmann exploration
------------------------------
Method for addressing the explore-exploit dilemma and choosing actions off-policy. Decrease the temperature over time. High temperatures incentivize exploration and low temperatures prioritize exploitation.

.. math::

    \pi(a|s) = e^{\frac{Q(s,a)}{\tau}}/e^{\sum_{a' \in A} \frac{Q(s,a')}{\tau}}

As :math:`\tau` approaches 0, it approximates a maximum function. As :math:`\tau` becomes large (above 3), the uniform distribution is approximated. The place where the distribution of probabilities is proportional to the differences in the Q-values does not appear to be fixed but usually occurs around 0.5.

Peng et al. (2016) started the temperature at 20 and linearly annealed it to 0.025 over 50,000 iterations.

Epsilon-greedy policy
------------------------
A method for inducing exploration during training. Choose the greedy policy with probability :math:`1-\epsilon` and a random action with probability :math:`\epsilon`.

Greedy policy
-----------------
A policy that always selects the best action according to its value or Q function without any regard for exploration.

Multiple Armed Bandit Problem
----------------------------------
Example of the explore-exploit tradeoff. Each machine (or bandit) provides a random reward from a probability distribution specific to that machine. A balance must be struck between picking the machine which has the best expected return (exploiting) and picking one's about which not much is known in the hope that they may be better (exploring).

The objective is to maximise the sum of the rewards over a set number of possible actions.

A simple approach that works well is to use an epsilon-greedy policy.

Off-policy and on-policy learning
-------------------------------------
In off-policy learning the behaviour distribution does not follow the policy. This allows a more exploratory behaviour distribution to be chosen, using an epsilon-greedy policy or Boltzmann exploration, for example.

In on-policy learning the policy determines the samples the network is trained on. An example is SARSA.

Thompson sampling
--------------------------
Method for addressing the explore-exploit dilemma. Samples randomly from a distribution over the parameters of the reward function and then selects what would be the optimal action if the sampled parameters were indeed the true ones.
