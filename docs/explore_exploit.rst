""""""""""""""""""""
Explore-exploit
""""""""""""""""""""

Boltzmann exploration
------------------------------
Method for addressing the explore-exploit dilemma and choosing actions off-policy, like the epsilon-greedy method. Decrease the temperature over time. High temperatures incentivize exploration and low temperatures prioritize exploitation.

.. math::

    \pi(a|s) = e^{\frac{Q(s,a)}{\tau}}/e^{\sum_{a' \in A} \frac{Q(s,a')}{\tau}}

As T approaches 0, it approximates a maximum function. As T becomes large (above 3), the uniform distribution is approximated. The place where the distribution of probabilities is proportional to the differences in the Q-values does not appear to be fixed but usually occurs around 0.5.

Peng et al. (2016) started the temperature at 20 and linearly annealed it to 0.025 over 50,000 iterations.

Epsilon-greedy policy
------------------------
Choose the greedy policy with probability :math:`1-\epsilon` and a random action with probability :math:`\epsilon`. Can be used to select the behaviour distribution.

Multiple Armed Bandit Problem
----------------------------------
Example of the explore-exploit tradeoff. Each machine provides a random reward from a probability distribution specific to that machine.

A simple approach that works well is to use an epsilon-greedy policy to choose the current best machine with probability :math:`1-\epsilon` and a random machine with probability :math:`\epsilon`.

Thompson sampling
--------------------------
Method for addressing the explore-exploit dilemma. Samples randomly from a distribution over the parameters of the reward function and then selects what would be the optimal action if the sampled parameters were indeed the true ones.
