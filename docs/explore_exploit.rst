""""""""""""""""""""""""""""""""""""""""
The explore-exploit dilemma
""""""""""""""""""""""""""""""""""""""""

Acquisition functions
-------------------------
An acquisition function defines how to balance exploration and exploitation when deciding the next point to sample.

https://www.cse.wustl.edu/~garnett/cse515t/spring_2015/files/lecture_notes/12.pdf

Probability of improvement
'''''''''''''''''''''''''''
Pick the action which maximises the chance of getting to a state with a value greater than the current best state.

Expected improvement
''''''''''''''''''''''
Pick the action which maximises the expected improvement of that new state over the current best state.

Upper confidence bound
'''''''''''''''''''''''''''

Boltzmann exploration
------------------------------
Method for addressing the explore-exploit dilemma and choosing actions off-policy, like the epsilon-greedy method. Decrease the temperature over time. High temperatures incentivize exploration and low temperatures prioritize exploitation.

.. math::

    \pi(a|s) = e^{\frac{Q(s,a)}{\tau}}/e^{\sum_{a' \in A} \frac{Q(s,a')}{\tau}}

As T approaches 0, it approximates a maximum function. As T becomes large (above 3), the uniform distribution is approximated. The place where the distribution of probabilities is proportional to the differences in the Q-values does not appear to be fixed but usually occurs around 0.5.

Peng et al. (2016) started the temperature at 20 and linearly annealed it to 0.025 over 50,000 iterations.

Epsilon-greedy policy
------------------------
Choose the greedy policy with probability :math:`1-\epsilon` and a random action with probability :math:`\epsilon`. Can be used to select the behaviour distribution.

Greedy policy
-----------------
A policy that always selects the best action according to its value function without any regard for exploration.

Multiple Armed Bandit Problem
----------------------------------
Example of the explore-exploit tradeoff. Each machine (or bandit) provides a random reward from a probability distribution specific to that machine. A balance must be struck between picking the machine which has the best expected return (exploiting) and picking one's about which not much is known in the hope that they may be better (exploring).

The objective is to maximise the sum of the rewards over a set number of possible actions.

A simple approach that works well is to use an epsilon-greedy policy.

Thompson sampling
--------------------------
Method for addressing the explore-exploit dilemma. Samples randomly from a distribution over the parameters of the reward function and then selects what would be the optimal action if the sampled parameters were indeed the true ones.
